{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Alexey Tkachenko BS22 AAI-02`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
    "\n",
    "- solving a problem of n-grams frequencies storing for a large corpus;\n",
    "- taking into account keyboard layout and associated misspellings;\n",
    "- efficiency improvement to make the solution faster;\n",
    "- ...\n",
    "\n",
    "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
    "\n",
    "##### IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas nltk autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "I decided to use the provided datasets for bigrams. The I picked were the `bigrams.txt` and `coca_all_links`. I didn't quite get the extra info that is present in the `coca_all_links`, but decided to use it anyway. Just without the unknown columns.\n",
    "\n",
    "No funny business here, just reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_datasets(datadir: str):\n",
    "    df = pd.DataFrame(columns=[\"count\", \"w1\", \"w2\"])\n",
    "    for path in os.listdir(datadir):\n",
    "        filepath = os.path.join(datadir, path)\n",
    "\n",
    "        new_df = pd.read_csv(\n",
    "            filepath,\n",
    "            sep=\"\\t\",\n",
    "            usecols=[0, 1, 2],\n",
    "            names=[\"count\", \"w1\", \"w2\"],\n",
    "            encoding=\"latin-1\",\n",
    "        )\n",
    "\n",
    "        new_df[\"w1\"] = new_df[\"w1\"].str.lower()\n",
    "        new_df[\"w2\"] = new_df[\"w2\"].str.lower()\n",
    "\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    return df, df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>a-national</td>\n",
       "      <td>rank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>buildings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176689</th>\n",
       "      <td>24</td>\n",
       "      <td>zviad</td>\n",
       "      <td>gamsakhurdia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176690</th>\n",
       "      <td>25</td>\n",
       "      <td>zweimal</td>\n",
       "      <td>leben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176691</th>\n",
       "      <td>24</td>\n",
       "      <td>zwick</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176692</th>\n",
       "      <td>24</td>\n",
       "      <td>zydeco</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176693</th>\n",
       "      <td>72</td>\n",
       "      <td>zz</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1176694 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        count          w1            w2\n",
       "0          36  a-national          rank\n",
       "1          92   abandoned      building\n",
       "2         139   abandoned     buildings\n",
       "3          50   abandoned           car\n",
       "4          47   abandoned          cars\n",
       "...       ...         ...           ...\n",
       "1176689    24       zviad  gamsakhurdia\n",
       "1176690    25     zweimal         leben\n",
       "1176691    24       zwick           and\n",
       "1176692    24      zydeco         music\n",
       "1176693    72          zz           top\n",
       "\n",
       "[1176694 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, total_count = load_datasets('data')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also return the `total_count` that I will use for probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate selection algorithm\n",
    "\n",
    "Initially I used Norvig's approach for candidate selection. It was sufficient, but during testing I found that the even though the candidate selection algorithm is good, the quality of the dataset for known words matters most. I used `english_words` package to get a dictionary of real english words, but it was of low quality.\n",
    "\n",
    "While looking for a decent dataset, I found an amazing library called [autocorrect](https://github.com/filyp/autocorrect). I decided to implement the correction myself, but take their candidate selector. If you take a look at the source code, they take an approach similar to Norvig's:\n",
    "\n",
    "```py\n",
    "def get_candidates(self, word):\n",
    "    w = Word(word, self.lang, self.only_replacements)\n",
    "    if self.fast:\n",
    "        candidates = self.existing([word]) or self.existing(w.typos()) or [word]\n",
    "    else:\n",
    "        candidates = (\n",
    "            self.existing([word])\n",
    "            or self.existing(w.typos())\n",
    "            or self.existing(w.double_typos())\n",
    "            or [word]\n",
    "        )\n",
    "    return [(self.nlp_data.get(c, 0), c) for c in candidates]\n",
    "```\n",
    "\n",
    "I decided to go forward with this approach, since it showed much better error detection rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autocorrect\n",
    "spell = autocorrect.Speller()\n",
    "def candidates(word: str):\n",
    "    return sorted(spell.get_candidates(word), key=lambda e: e[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have a function that utilized the bigram dataset we've loaded earlier. I basically query the dataframe for bigrams based on two words, and return the probability. I normalize probabilities using `np.log10`, which I read about [here](https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model).\n",
    "\n",
    "I build an index to save on computations. Surprizingly, this works much faster than I expected. The lookup speed on `pd.DataFrame()` is outstanding!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_index(df: pd.DataFrame):\n",
    "    df[\"comb\"] = df[\"w1\"] + df[\"w2\"]\n",
    "\n",
    "    return (\n",
    "        df.drop([\"w1\", \"w2\"], axis=1).set_index([\"comb\"]).drop_duplicates(keep=\"first\")\n",
    "    )\n",
    "\n",
    "\n",
    "index = build_index(df)\n",
    "\n",
    "\n",
    "def prob(w1, w2):\n",
    "    try:\n",
    "        return np.log10(index[w1 + w2][0] / total_count)\n",
    "    except KeyError:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the algorithm is as follows:\n",
    "\n",
    "- We pad the sentence with `[BOS]` and `[EOS]` tokens for convenience\n",
    "- We generate bigrams \n",
    "- Iterating through bigrams, take the candidate with the highest score\n",
    "- Take top 3 candidates\n",
    "- Calculate the probability of new bigrams with one of the new candidates replacing the first word\n",
    "- Take most probable one\n",
    "\n",
    "Also:\n",
    "- Compute the same thing, but for the second word in the bigram (replacing the second word instead of the first)\n",
    "- Save the this candidate for the next step of the loop\n",
    "- During the next iteration, if the candidate from the previous step is more probable, make the change\n",
    "\n",
    "This addition helps the algorithm \"look\" at the word from \"both sides\" and helped in some cases during testing. The bigram might have looked ok from the left, but from the right it was incorrect. That is why I have implemented this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we all be doing sport and you need to leave their doing man\n",
      "ok i understood your point but it is still hard for me to believe that he made it\n",
      "does not divide the sunday from the week\n",
      "good now sit down and tell me he that knows\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def correct(sentence: str, n=2, b=3):\n",
    "    w2_incorrect_best_for_w1 = (-np.inf, \"\")\n",
    "\n",
    "    # words = [\"[BOS]\"] + sentence.lower().split() + [\"[EOS]\"]\n",
    "    words =  [\"[BOS]\"] + sentence.strip().lower().split() + [\n",
    "        \"[EOS]\"\n",
    "    ]  # Assuming the first and last words are correct (for now)\n",
    "    ngrms = list(ngrams(words, n))\n",
    "\n",
    "    correct_sequience = []\n",
    "    for gram in ngrms:\n",
    "        w1, w2 = gram\n",
    "\n",
    "\n",
    "        cands = []\n",
    "        for cand in sorted(candidates(w1), key=lambda e: e[0], reverse=True)[\n",
    "            :b\n",
    "        ]:  # Top 3 candidates\n",
    "            _, c = cand\n",
    "            cands.append((prob(c, w2), c))\n",
    "        cands = sorted(cands, key=lambda e: e[0], reverse=True)\n",
    "\n",
    "        # print(f\"{w1} {w2} | {cands}\")\n",
    "\n",
    "        correct_sequience += [cands[0][1] if cands[0][0] > w2_incorrect_best_for_w1[0] else w2_incorrect_best_for_w1[1]]\n",
    "            \n",
    "\n",
    "        cands = []\n",
    "        for cand in sorted(candidates(w2), key=lambda e: e[0], reverse=True)[\n",
    "            :b\n",
    "        ]:  # Top 3 candidates\n",
    "            _, c = cand\n",
    "            cands.append((prob(w1, c), c))\n",
    "        cands = sorted(cands, key=lambda e: e[0], reverse=True)\n",
    "\n",
    "        # print(f\"{w1} {w2} | {cands}\")\n",
    "        \n",
    "        w2_incorrect_best_for_w1 = cands[0]\n",
    "\n",
    "        # correct_sequience += [cands[0][1]]\n",
    "            \n",
    "            \n",
    "\n",
    "    return \" \".join(correct_sequience).strip()\n",
    "\n",
    "\n",
    "print(correct(\"we wll be dking sport and yow need to leave their dking man\"))\n",
    "print(correct(\"ok i understood your poimr but it is stoll hard for me to believe rhat he made it\"))\n",
    "print(correct(\"does not duvude the sinsay from the week\"))\n",
    "print(correct(\"Goof now sit drwn and tell me, he that lnows\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "\n",
    "WORDS = Counter(words(open(\"big.txt\").read()))\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates_n(word), key=P)\n",
    "\n",
    "\n",
    "def candidates_n(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errored out:\n",
      "djsasters ins the sun and gtlhe moist star\n",
      "N-grams\n",
      "disasters ins the sun and the moist star\n",
      "Norvig\n",
      "disasters in the sun and the moist star\n",
      "Original:\n",
      "disasters in the sun and the moist star\n",
      "\n",
      "Errored out:\n",
      "upon whaose influence neptunes empire stands\n",
      "N-grams\n",
      "upon whose influence neptunes empire stands\n",
      "Norvig\n",
      "upon whose influence neptunes empire stands\n",
      "Original:\n",
      "upon whose influence neptunes empire stands\n",
      "\n",
      "Errored out:\n",
      "uals sick almost twq geomsday wrth eclipse\n",
      "N-grams\n",
      "urls sick almost two doomsday with eclipse\n",
      "Norvig\n",
      "pals sick almost two geomsday with eclipses\n",
      "Original:\n",
      "was sick almost to doomsday with eclipse\n",
      "\n",
      "Errored out:\n",
      "and even the like precurse of fierce events\n",
      "N-grams\n",
      "and even the like precise of fierce events\n",
      "Norvig\n",
      "and even the like recourse of fierce events\n",
      "Original:\n",
      "and even the like precurse of fierce events\n",
      "\n",
      "Errored out:\n",
      "as hqrbingeos precqeding nstinl the fates\n",
      "N-grams\n",
      "as hqrbingeos preceding still the fates\n",
      "Norvig\n",
      "as harbingers preceding still the fates\n",
      "Original:\n",
      "as harbingers preceding still the fates\n",
      "\n",
      "Errored out:\n",
      "aez prologue to the omfen coming on\n",
      "N-grams\n",
      "rez prologue to the omen coming on\n",
      "Norvig\n",
      "aet prologue to the omen coming on\n",
      "Original:\n",
      "and prologue to the omen coming on\n",
      "\n",
      "Errored out:\n",
      "have heaven and earth ztosgether demonstrated\n",
      "N-grams\n",
      "have heaven and earth together demonstrated\n",
      "Norvig\n",
      "have heaven and earth together demonstrated\n",
      "Original:\n",
      "have heaven and earth together demonstrated\n",
      "\n",
      "Errored out:\n",
      "unto ojk climatures and countrymen\n",
      "N-grams\n",
      "unto ok climates and countrymen\n",
      "Norvig\n",
      "unto oak ligatures and countrymen\n",
      "Original:\n",
      "unto our climatures and countrymen\n",
      "\n",
      "Errored out:\n",
      "but soft bbcold lo wherejs it comews again\n",
      "N-grams\n",
      "but soft bold lo whereas it comes again\n",
      "Norvig\n",
      "but soft cold lo whereas it comes again\n",
      "Original:\n",
      "but soft behold lo where it comes again\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess(text: list[str]):\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        new_text.append(re.sub(r\"[^A-Za-z0-9 ]+\", \"\", line.lower()).strip())\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def noizyfy(text: list[str], error_rate=0.25, error_type_rate=0.25):\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        words = line.split()\n",
    "\n",
    "        new_words = []\n",
    "        for w in words:\n",
    "\n",
    "            def mutate(word):\n",
    "                def edits1(word):\n",
    "                    \"All edits that are one edit away from `word`.\"\n",
    "                    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "                    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "                    deletes = [L + R[1:] for L, R in splits if R]\n",
    "                    transposes = [\n",
    "                        L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1\n",
    "                    ]\n",
    "                    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "                    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "                    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "                def edits2(word):\n",
    "                    \"All edits that are two edits away from `word`.\"\n",
    "                    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "                if random.random() < error_type_rate:\n",
    "                    return random.choice(list(edits1(word)))\n",
    "                else:\n",
    "                    return random.choice(list(edits2(word)))\n",
    "\n",
    "            if random.random() < error_rate:\n",
    "                new_words.append(mutate(w))\n",
    "            else:\n",
    "                new_words.append(w)\n",
    "\n",
    "        new_text.append(\" \".join(new_words))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "# Original text: https://shakespeare.mit.edu/hamlet/full.html\n",
    "shakespeare_text = [\n",
    "    \"Disasters in the sun; and the moist star\",\n",
    "    \"Upon whose influence Neptune's empire stands\",\n",
    "    \"Was sick almost to doomsday with eclipse:\",\n",
    "    \"And even the like precurse of fierce events,\",\n",
    "    \"As harbingers preceding still the fates\",\n",
    "    \"And prologue to the omen coming on,\",\n",
    "    \"Have heaven and earth together demonstrated\",\n",
    "    \"Unto our climatures and countrymen.--\",\n",
    "    \"But soft, behold! lo, where it comes again!\",\n",
    "]\n",
    "\n",
    "\n",
    "text = preprocess(shakespeare_text)\n",
    "\n",
    "\n",
    "for original, fake in zip(text, noizyfy(text)):\n",
    "    print(\"Errored out:\")\n",
    "    print(fake)\n",
    "    print(\"N-grams\")\n",
    "    print(correct(fake))\n",
    "    print(\"Norvig\")\n",
    "    print(\" \".join(correction(word) for word in fake.split()))\n",
    "    print(\"Original:\")\n",
    "    print(original)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On synthetic errors both models perform very poorly. Let's try something more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errored out:\n",
      "the act leaped gracefully onto he windowsill its tail flicking in the sunlight\n",
      "N-grams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the act leased peacefully onto he windowsill its tail clicking in the sunlight\n",
      "Norvig\n",
      "the act leaped gracefully onto he windowsill its tail flicking in the sunlight\n",
      "Original:\n",
      "the cat leaped gracefully onto the windowsill its tail flicking in the sunlight\n",
      "\n",
      "Errored out:\n",
      "shw decided to bake a cake even though she had never triex the rceipe before\n",
      "N-grams\n",
      "sha decided to bake a cake even though she had never tried the recipe before\n",
      "Norvig\n",
      "she decided to bake a cake even though she had never tried the recipe before\n",
      "Original:\n",
      "she decided to bake a cake even though she had never tried the recipe before\n",
      "\n",
      "Errored out:\n",
      "the old bookstore smelled of duts and memories with shelves stacked to the ceiling\n",
      "N-grams\n",
      "the old bookstore spelled of duty and memories with shelves stacked to the ceiling\n",
      "Norvig\n",
      "the old bookstore smelled of duty and memories with shelves stacked to the ceiling\n",
      "Original:\n",
      "the old bookstore smelled of dust and memories with shelves stacked to the ceiling\n",
      "\n",
      "Errored out:\n",
      "he couldnt believe is luck when he found x twentydollar bill on the sidewalk\n",
      "N-grams\n",
      "he couldnt believe is luck when he found x twentydollar bill on the sidewalk\n",
      "Norvig\n",
      "he couldn believe is luck when he found x twentydollar bill on the sidewalk\n",
      "Original:\n",
      "he couldnt believe his luck when he found a twentydollar bill on the sidewalk\n",
      "\n",
      "Errored out:\n",
      "the storm rolled in quickly turning the sky a deep shade of gray\n",
      "N-grams\n",
      "the storm rolled in quickly turning the sky a deep shade of gray\n",
      "Norvig\n",
      "the storm rolled in quickly turning the sky a deep shade of gray\n",
      "Original:\n",
      "the storm rolled in quickly turning the sky a deep shade of gray\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adj_keys = {\n",
    "    \"q\": [\"w\", \"a\", \"s\"],\n",
    "    \"w\": [\"q\", \"e\", \"a\", \"s\", \"d\"],\n",
    "    \"e\": [\"w\", \"r\", \"s\", \"d\", \"f\"],\n",
    "    \"r\": [\"e\", \"t\", \"d\", \"f\", \"g\"],\n",
    "    \"t\": [\"r\", \"y\", \"f\", \"g\", \"h\"],\n",
    "    \"y\": [\"t\", \"u\", \"g\", \"h\", \"j\"],\n",
    "    \"u\": [\"y\", \"i\", \"h\", \"j\", \"k\"],\n",
    "    \"i\": [\"u\", \"o\", \"j\", \"k\", \"l\"],\n",
    "    \"o\": [\"i\", \"p\", \"k\", \"l\"],\n",
    "    \"p\": [\"o\", \"l\"],\n",
    "    \"a\": [\"q\", \"w\", \"s\", \"z\", \"x\"],\n",
    "    \"s\": [\"a\", \"w\", \"e\", \"d\", \"z\", \"x\", \"c\"],\n",
    "    \"d\": [\"s\", \"e\", \"r\", \"f\", \"x\", \"c\", \"v\"],\n",
    "    \"f\": [\"d\", \"r\", \"t\", \"g\", \"c\", \"v\", \"b\"],\n",
    "    \"g\": [\"f\", \"t\", \"y\", \"h\", \"v\", \"b\", \"n\"],\n",
    "    \"h\": [\"g\", \"y\", \"u\", \"j\", \"b\", \"n\", \"m\"],\n",
    "    \"j\": [\"h\", \"u\", \"i\", \"k\", \"n\", \"m\"],\n",
    "    \"k\": [\"j\", \"i\", \"o\", \"l\", \"m\"],\n",
    "    \"l\": [\"k\", \"o\", \"p\"],\n",
    "    \"z\": [\"a\", \"s\", \"x\"],\n",
    "    \"x\": [\"z\", \"s\", \"d\", \"c\"],\n",
    "    \"c\": [\"x\", \"d\", \"f\", \"v\"],\n",
    "    \"v\": [\"c\", \"f\", \"g\", \"b\"],\n",
    "    \"b\": [\"v\", \"g\", \"h\", \"n\"],\n",
    "    \"n\": [\"b\", \"h\", \"j\", \"m\"],\n",
    "    \"m\": [\"n\", \"j\", \"k\"],\n",
    "}\n",
    "\n",
    "\n",
    "def noizyfy(text: list[str], error_rate=0.10):\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        words = line.split()\n",
    "\n",
    "        new_words = []\n",
    "        for w in words:\n",
    "\n",
    "            def mutate(word):\n",
    "                def edits1(word):\n",
    "                    \"All edits that are one edit away from `word`.\"\n",
    "                    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "                    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "                    deletes = [L + R[1:] for L, R in splits if R]\n",
    "                    transposes = [\n",
    "                        L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1\n",
    "                    ]\n",
    "                    replaces = [\n",
    "                        L + random.choice(adj_keys[R[0]]) + R[1:]\n",
    "                        for L, R in splits\n",
    "                        if R\n",
    "                    ]\n",
    "                    return set(deletes + transposes + replaces)\n",
    "\n",
    "                # def edits2(word):\n",
    "                #     \"All edits that are two edits away from `word`.\"\n",
    "                #     return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "                # if random.random() < error_type_rate:\n",
    "                return random.choice(list(edits1(word)))\n",
    "                # else:\n",
    "                #     return random.choice(list(edits2(word)))\n",
    "\n",
    "            if random.random() < error_rate:\n",
    "                new_words.append(mutate(w))\n",
    "            else:\n",
    "                new_words.append(w)\n",
    "\n",
    "        new_text.append(\" \".join(new_words))\n",
    "    return new_text\n",
    "\n",
    "\n",
    "text = [\n",
    "    \"The cat leaped gracefully onto the windowsill, its tail flicking in the sunlight.\",\n",
    "    \"She decided to bake a cake, even though she had never tried the recipe before.\",\n",
    "    \"The old bookstore smelled of dust and memories, with shelves stacked to the ceiling.\",\n",
    "    \"He couldn’t believe his luck when he found a twenty-dollar bill on the sidewalk.\",\n",
    "    \"The storm rolled in quickly, turning the sky a deep shade of gray.\",\n",
    "]\n",
    "\n",
    "\n",
    "text = preprocess(text)\n",
    "\n",
    "\n",
    "for original, fake in zip(text, noizyfy(text)):\n",
    "    print(\"Errored out:\")\n",
    "    print(fake)\n",
    "    print(\"N-grams\")\n",
    "    print(correct(fake))\n",
    "    print(\"Norvig\")\n",
    "    print(\" \".join(correction(word) for word in fake.split()))\n",
    "    print(\"Original:\")\n",
    "    print(original)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "The ngram language model seem to do a better job if we consider cases closer to real-life with typing errors, though the Norvig's approach is not so far behind. To me, it seems like that for the n-gram model the quality of the n-gram dataset and the dictionary of known words matter significantly. Regardless, I believe that we should look towards more advanced solutions like transformers, since the capture the meaning of tokens, not just their frequency in text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
